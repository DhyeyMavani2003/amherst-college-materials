---
title: "HW6"
output: word_document
date: "2023-04-20"
---




## HW6


1. Do problem 11.2 on p. 246 of the textbook.


Randomized block design: Researchers interested in identifying the optimal planting density for a type of perennial grass performed the following
randomized experiment: Ten different plots of land were each divided into
eight subplots, and planting densities of 2, 4, 6 and 8 plants per square meter were randomly assigned to the subplots, so that there are two subplots
at each density in each plot. At the end of the growing season the amount of plant matter yield was recorded in metric tons per hectare. These data
appear in the file pdensity.dat. The researchers want to fit a model like y = β1 + β2x + β3x^2 + e, where y is yield and x is planting density, but
worry that since soil conditions vary across plots they should allow for some across-plot heterogeneity in this relationship. To accommodate this
possibility we will analyze these data using the hierarchical linear model described in Section 11.1.


a) Before we do a Bayesian analysis we will get some ad hoc estimates of these parameters via least squares regression. Fit the model y =
β1+β2x+β3x^2+e using OLS for each group, and make a plot showing the heterogeneity of the least squares regression lines. From the least squares coefficients find ad hoc estimates of θ and Σ. Also obtain an estimate of σ2 by combining the information from the residuals across the groups.


```{r}
prob1 <- read.csv("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/pdensity.dat",sep="")

prob1[prob1$plot==1,]$yield
ids<-sort(unique(prob1$plot)) 
m<-length(ids)
Y<-list() ; X<-list() ; N<-NULL
for(j in 1:m) 
{
  Y[[j]]<-prob1[prob1$plot==ids[j], 3] 
  N[j]<- sum(prob1$plot==ids[j])
  xj<-prob1[prob1$plot==ids[j], 2] 
  #xj<-(xj-mean(xj))
  X[[j]]<-cbind( rep(1,N[j]), xj , xj^2 )
}



#### OLS fits
S2.LS<-BETA.LS<-NULL
for(j in 1:m) {
  fit<-lm(Y[[j]]~-1+X[[j]] )
  BETA.LS<-rbind(BETA.LS,c(fit$coef)) 
  S2.LS<-c(S2.LS, summary(fit)$sigma^2) 
} 

par(mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
par(mfrow=c(2,2))
density_points <- c(2,4,6,8)
BETA.MLS<-apply(BETA.LS,2,mean)

plot(density_points, BETA.MLS[1]+BETA.MLS[2]*density_points+BETA.MLS[3]*density_points*density_points, range(prob1[,2]),range(prob1[,3]),type="b",xlab="density", 
      ylab="yield")
for(j in 1:m) {    points(density_points, BETA.LS[j,1]+BETA.LS[j,2]*density_points+(BETA.LS[j,3]*density_points*density_points),type="b",col="gray")  }

#BETA.MLS<-apply(BETA.LS,2,mean)
#points(density_points, BETA.MLS[1]+BETA.MLS[2]*density_points+BETA.MLS[3]*density_points*density_points)
#abline(BETA.MLS[1],BETA.MLS[2],lwd=2)

plot(N,BETA.LS[,1],xlab="sample size",ylab="intercept")
abline(h= BETA.MLS[1],col="black",lwd=2)
plot(N,BETA.LS[,2],xlab="sample size",ylab="slope of density")
abline(h= BETA.MLS[2],col="black",lwd=2)
plot(N,BETA.LS[,3],xlab="sample size",ylab="slope of density^2")
abline(h= BETA.MLS[3],col="black",lwd=2)
```


An estimate of $\Sigma$ is the covariance matrix with the values obtained in the S2.LS. An estimate for $\Theta$ is the mean of the calculated OLS beta vector BETA.MLS. An estimate for $\sigma^2$ is the mean of the s2.LS list




```{r}
paste("The estimate for theta is ",BETA.MLS[1], "for the intercept term, ",BETA.MLS[2]," for the density term, and ", BETA.MLS[3], " for the density^2 term")
SIGMA <- cov(BETA.LS)
sigma <- mean(S2.LS)
paste("The estimate for sigma2 is ", sigma )

```

There is clearly different intercept and slope estimates across the different possible regression lines on the above plot, signifying that there is heterogeneity.



b) Now we will perform an analysis of the data using the following distributions as prior distributions:
Σ−1 ∼ Wishart(4, Σˆ−1)
θ ∼ multivariate normal(θˆ, Σˆ)
σ2 ∼ inverse − gamma(1, σˆ2)
where θˆ, Σ, ˆ σˆ2 are the estimates you obtained in a). Note that this analysis is not combining prior information with information from
the data, as the“prior” distribution is based on the observed data. Exercises 247
However, such an analysis can be roughly interpreted as the Bayesian analysis of an individual who has weak but unbiased prior information.

```{r}
## mvnormal simulation
rmvnorm<-function(n,mu,Sigma)
{ 
  E<-matrix(rnorm(n*length(mu)),n,length(mu))
  t(  t(E%*%chol(Sigma)) +c(mu))
}

## Wishart simulation
rwish<-function(n,nu0,S0)
{
  sS0 <- chol(S0)
  S<-array( dim=c( dim(S0),n ) )
  for(i in 1:n)
  {
    Z <- matrix(rnorm(nu0 * dim(S0)[1]), nu0, dim(S0)[1]) %*% sS0
    S[,,i]<- t(Z)%*%Z
  }
  S[,,1:n]
}

## Setup
p<-dim(X[[1]])[2]
mu0 = apply(BETA.LS,2,mean)
theta<-mu0  # set the theta and prior for 
nu0<-1 ; s2<-s20<-mean(S2.LS)
eta0<-4
Sigma<-S0<-L0<-cov(BETA.LS)
BETA<-BETA.LS
THETA.b<-S2.b<-NULL
iL0<-solve(L0) ; iSigma<-solve(Sigma)
Sigma.ps<-matrix(0,p,p)
SIGMA.PS<-NULL
BETA.ps<-BETA*0
BETA.pp<-NULL
#set.seed(1)
#mu0[2]+c(-1.96,1.96)*sqrt(L0[2,2])

## MCMC
for(s in 1:10000) {
  ##update beta_j 
  for(j in 1:m) 
  {  
    Vj<-solve( iSigma + t(X[[j]])%*%X[[j]]/s2 )
    Ej<-Vj%*%( iSigma%*%theta + t(X[[j]])%*%Y[[j]]/s2 )
    BETA[j,]<-rmvnorm(1,Ej,Vj) 
  } 
  ##
  
  ##update theta
  Lm<-  solve( iL0 +  m*iSigma )
  mum<- Lm%*%( iL0%*%mu0 + iSigma%*%apply(BETA,2,sum))
  theta<-t(rmvnorm(1,mum,Lm))
  ##
  
  ##update Sigma
  mtheta<-matrix(theta,m,p,byrow=TRUE)
  iSigma<-rwish(1, eta0+m, solve( S0+t(BETA-mtheta)%*%(BETA-mtheta) ) )
  ##
  
  ##update s2
  RSS<-0
  for(j in 1:m) { RSS<-RSS+sum( (Y[[j]]-X[[j]]%*%BETA[j,] )^2 ) }
  s2<-1/rgamma(1,(nu0+sum(N))/2, (nu0*s20+RSS)/2 )
  ##
  ##store results
  if(s%%10==0) 
  { 
    #cat(s,s2,"\n")
    S2.b<-c(S2.b,s2);THETA.b<-rbind(THETA.b,t(theta))
    Sigma.ps<-Sigma.ps+solve(iSigma) ; BETA.ps<-BETA.ps+BETA
    SIGMA.PS<-rbind(SIGMA.PS,c(solve(iSigma)))
    BETA.pp<-rbind(BETA.pp,rmvnorm(1,theta,solve(iSigma)) )
  }
  ##
}
```



c) Use a Gibbs sampler to approximate posterior expectations of β for each group j, and plot the resulting regression lines. Compare to the regression lines in a) above and describe why you see any differences between the two sets of regression lines.


```{r}
## MCMC diagnostics
library(coda)
effectiveSize(S2.b)
effectiveSize(THETA.b[,1])
effectiveSize(THETA.b[,2])
effectiveSize(THETA.b[,3])
apply(SIGMA.PS,2,effectiveSize)
#par(mfrow=c(6,2))
tmp<-NULL;for(j in 1:dim(SIGMA.PS)[2]) { tmp<-c(tmp,acf(SIGMA.PS[,j])$acf[2]) }

acf(S2.b)
acf(THETA.b[,1])
acf(THETA.b[,2])

#par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))

plot(density(THETA.b[,2],adj=2),xlim=range(BETA.pp[,2]), 
     main="",xlab="slope parameter",ylab="posterior density",lwd=2)
lines(density(BETA.pp[,2],adj=2),col="gray",lwd=2)
legend( -3 ,1.0 ,legend=c( expression(theta[2]),expression(tilde(beta)[2])), 
        lwd=c(2,2),col=c("black","gray"),bty="n") 

quantile(THETA.b[,2],prob=c(.025,.5,.975))
mean(BETA.pp[,2]<0) 

BETA.PM<-BETA.ps/1000

density_points <- c(2,4,6,8)
plot(density_points,BETA.MLS[1]+BETA.MLS[2]*density_points + BETA.MLS[3]*density_points*density_points ,type = "b",ylab="Yield")
for(j in 1:m) {    points(density_points,BETA.PM[j,1]+BETA.PM[j,2]*density_points + BETA.PM[j,3]*density_points*density_points ,type = "b",col="gray")  }


```


Checking the MCMC diagnostics it seems that the effective sample sizes of all of the parameters are somewhat large, and the autocorrelation plots do not suggest that the Markov Chain has much autocorrelation so I find the chain to be a reasonable sample.


```{r}
par(mfrow=c(1,2))
plot(density_points,BETA.MLS[1]+BETA.MLS[2]*density_points + BETA.MLS[3]*density_points*density_points ,type = "b",ylab="Yield",xlab="density_points",ylim = c(5,10))
for(j in 1:m) {    points(density_points,BETA.PM[j,1]+BETA.PM[j,2]*density_points + BETA.PM[j,3]*density_points*density_points ,type = "b",col="gray")  }

plot(density_points, BETA.MLS[1]+BETA.MLS[2]*density_points+BETA.MLS[3]*density_points*density_points,xlab="density_points", type = "b",ylab="Yield", ylim = c(5,10))
for(j in 1:m) {    points(density_points, BETA.LS[j,1]+BETA.LS[j,2]*density_points+(BETA.LS[j,3]*density_points*density_points),col="gray" , type = "b" )}

```


The bayesian analysis shows that there is clearly a change in the expected yield at higher densities depending on which plot is being used for planting. This is known from the OLS estimation as well, though there is shown to be more heteroscedasticity in the bayesian analysis. This suggests that the impact of the plot is more extreme as density increases. 

The Bayesian method also shows shrinkage towards the mean for all of the different plots as can be seen from comparing the two plots as well. This is to be expected as the Bayesian estimates share prior knowledge. 


d) From your posterior samples, plot marginal posterior and prior densities of θ and the elements of Σ. Discuss the evidence that the slopes or intercepts vary across groups.

priors: 

θ ∼ multivariate normal(θˆ, Σˆ)
Σ−1 ∼ Wishart(4, Σˆ−1)

```{r}
theta_prior_sample <- rmvnorm(1000,mu0,solve(cov(BETA.LS)))
#par(mfrow=c(3,1))
plot(density(theta_prior_sample[,1]),ylim=c(0,1))
lines(density(THETA.b[,1]))
plot(density(theta_prior_sample[,2]),ylim=c(0,3))
lines(density(THETA.b[,2]))
plot(density(theta_prior_sample[,3]),xlim=c(-2,2),ylim=c(0,20))
lines(density(THETA.b[,3]))
```
The marginal posterior densities for all three theta parameters have substantially less variability as seen within the plots above.



Looking at the marginal posterior distributions of the elements of the sigma matrix and comparing them with the averages of each element:

```{r}
prior_isigma <- rwish(1000,4,solve(cov(BETA.LS)))
#prior_isigma[,,1]
#solve(prior_isigma[,,1])
PRIOR_SIGMA <- NULL
for(i in 1:1000){
  #prior_sigma <- solve(prior_isigma[,,i])
  PRIOR_SIGMA <- rbind(PRIOR_SIGMA,c(solve(prior_isigma[,,i])))
}

par(mfrow=c(3,3))
for(j in 1:9){
plot(density(SIGMA.PS[,j]),main = paste("Density of Sigma entry ", j))
lines(density(PRIOR_SIGMA[,j]),col="gray")}

```


As the posterior sigma matrix "shrinks" in comparison to the prior this implies that the plot does have an effect on yield. This also suggests that the density  and the density^2 parameters are explained more by the fixed effect term than the random effect of the plot.  



e) Suppose we want to identify the planting density that maximizes average yield over a random sample of plots. Find the value xmax of x
that maximizes expected yield, and provide a 95% posterior predictive interval for the yield of a randomly sampled plot having planting
density xmax.

```{r}
paste("The density that maximizes the average yield is ",mean(BETA.pp[,2])/(-2*mean(BETA.pp[,3])))
x_max <- mean(BETA.pp[,2])/(-2*mean(BETA.pp[,3]))
y_pred_prob1 <- BETA.pp[,1]+ (BETA.pp[,2]*x_max) + (BETA.pp[,3]*x_max*x_max)
paste("A 95% posterior predictive interval for the yield of a randomly sampled plot having planting
density xmax is (", quantile(y_pred_prob1,0.025),", ",quantile(y_pred_prob1,0.975),")" )
```




2) Do problem 12.1 on p. 249 of the textbook.

Rank regression: The 1996 General Social Survey gathered a wide variety of information on the adult U.S. population, including each survey
respondent’s sex, their self-reported frequency of religious prayer (on a six-level ordinal scale), and the number of items correct out of 10 on a
short vocabulary test. These data appear in the file prayer.dat. Using the rank regression procedure described in Section 12.1.2, estimate the
parameters in a regression model for Yi= prayer as a function of xi,1 = sex of respondent (0-1 indicator of being female) and xi,2 = vocabulary
score, as well as their interaction xi,3 = xi,1 × xi,2. Compare marginal prior distributions of the three regression parameters to their posterior distributions, and comment on the evidence that the relationship between prayer and score differs across the sexes



```{r}
prob2 <- read.csv("http://www2.stat.duke.edu/~pdh10/FCBS/Exercises/prayer.dat",sep="")
#### Ordinal probit regression
X<-cbind(prob2$female,prob2$vocab,prob2$female.vocab)
y<- prob2$prayer
#keep<- (1:length(y))[ !is.na( apply( cbind(X,y),1,mean) ) ]
#X<-X[keep,] ; y<-y[keep]
ranks<-match(y,sort(unique(y))) ; uranks<-sort(unique(ranks))
n<-dim(X)[1] ; p<-dim(X)[2]
iXX<-solve(t(X)%*%X)  ; V<-iXX*(n/(n+1)) ; cholV<-chol(V)
## setup
#set.seed(1)
beta<-rep(0,p) 
z<-qnorm(rank(y,ties.method="random")/(n+1))
g<-rep(NA,length(uranks)-1)
K<-length(uranks)
BETA<-matrix(NA,1000,p) ; Z<-matrix(NA,1000,n) ; ac<-0
mu<-rep(0,K-1) ; sigma<-rep(100,K-1) 

## MCMC
S<-25000
for(s in 1:S) 
{
  #update g 
  for(k in 1:(K-1)) 
  {
    a<-max(z[y==k])
    b<-min(z[y==k+1])
    u<-runif(1, pnorm( (a-mu[k])/sigma[k] ),
             pnorm( (b-mu[k])/sigma[k] ) )
    g[k]<- mu[k] + sigma[k]*qnorm(u)
  }
  #update beta
  E<- V%*%( t(X)%*%z )
  beta<- cholV%*%rnorm(p) + E
  #update z
  ez<-X%*%beta
  a<-c(-Inf,g)[ match( y-1, 0:K) ]
  b<-c(g,Inf)[y]  
  u<-runif(n, pnorm(a-ez),pnorm(b-ez) )
  z<- ez + qnorm(u)
  
  
  #help mixing
  c<-rnorm(1,0,n^(-1/3))  
  zp<-z+c ; gp<-g+c
  lhr<-  sum(dnorm(zp,ez,1,log=T) - dnorm(z,ez,1,log=T) ) + 
    sum(dnorm(gp,mu,sigma,log=T) - dnorm(g,mu,sigma,log=T) )
  if(log(runif(1))<lhr) { z<-zp ; g<-gp ; ac<-ac+1 }
  
  if(s%%(S/1000)==0) 
  { 
    #cat(s/S,ac/s,"\n")
    BETA[s/(S/1000),]<-  beta
    Z[s/(S/1000),]<- z
  }
} 

plot(X[,1]+(X[,2]),Z[1000,],
     pch=15+X[,1],col=c("gray","black")[X[,2]+1],
     xlab="Vocab score",ylab="z", ylim=range(Z),
     xlim=c(0,9))

beta.pm<-apply(BETA,2,mean)
ZPM<-apply(Z,2,mean)
abline(0,beta.pm[2],lwd=2 ,col="gray")
abline(beta.pm[1],beta.pm[2]+beta.pm[3],col="black",lwd=2 )
legend(5,4,legend=c("female=0","female=1"),pch=c(15,16),col=c("gray","black"))


 

plot(density(BETA[,3],adj=2),lwd=2,xlim=c(-.5,.5),main="",
     xlab=expression(beta[3]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[3,3] )
x<-seq(-.7,.7,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")
```

```{r}
mean(BETA[,1])
mean(BETA[,2])
mean(BETA[,3])
```
Based on the output of the model and visualization of the two regression lines, it appears that the amount a person's prayer "score" changes based on how they score on a vocab test does change based on their gender, with the impact of scoring a point higher on the vocab test leading to an additional -0.084 decrease in a persons prayer frequency if they are female. This is supported by the 95% "Confidence Interval" of BETA[,1] which is the vector of the posterior values of the coefficient for the indicator female=1.

```{r}

quantile(BETA[,1],c(0.025,0.975))

```

This interval does not contain zero so it suggests there is a difference between female and male observations. 

Looking further into the question: if the relationship between prayer and score differs across the sexes?

It can be seen that the confidence interval for the posterior values of the interaction term female*score contains 0, which conventionally suggests that there is not enough evidence to state that the relationship between prayer and score differs across the sexes.
```{r}
quantile(BETA[,3],c(0.025,0.975))
```




Plots of the priors and posteriors of the regression parameters:


```{r}
par(mfrow=c(1,1))
plot(density(BETA[,1],adj=2),lwd=2,xlim=c(-2,2),main="",
     xlab=expression(beta[1]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[1,1] )
x<-seq(-2.5,2.5,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")

plot(density(BETA[,2],adj=2),lwd=2,xlim=c(-.5,.5),main="",
     xlab=expression(beta[2]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[2,2] )
x<-seq(-.7,.7,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")

plot(density(BETA[,3],adj=2),lwd=2,xlim=c(-.5,.5),main="",
     xlab=expression(beta[3]),ylab="density")
sd<-sqrt(  solve(t(X)%*%X/n)[3,3] )
x<-seq(-.7,.7,length=100)
lines(x,dnorm(x,0,sd),lwd=2,col="gray")
legend(-.5,6.5,legend=c("prior","posterior"),lwd=c(2,2),col=c("gray","black"),bty="n")
```




3. Consider the problem of simulating truncated normal distribution which was needed in the latent probit regression model.
f(x) = exp(-1/2sigma2*(x-mu)^2) I(a,b)


a) Prove theoretically that the method described on p. 212 generates the above
distribution.


b) Consider the following alternative algorithm:
1. Generate X~N(mu,sigma2)
 
2. If X is in (a,b) then accept it, otherwise reject it.

3. Repeat from step 1.

Suppose that mu = 8, sigma2 = 3, a = 0, b = 2 and we want to generate a sample of size n = 100. Implement the above algorithm. How many iterations did you need to achieve the desired sample size? Compare the results with an implementation of the algorithm in the textbook (compare the computation time and the
histograms).

```{r}
mu = 8
sigma2 = 3
a = 0
b = 2
iterations <- 0
n <- NULL
while(length(n)<100){
  iterations <- iterations+1
  x <- rnorm(1,mu,sqrt(sigma2))
  if(b-x-a>0 ){
    n<- rbind(n,x)
    
  }
}
paste("It took ",iterations, " iterations to get a sample of 100 values in the range of (a,b)")
```
Textbook algorithm:

1. Sample u~U(pnorm((a-mu)/sigma) , pnorm((b-mu)/sigma))
2. set x = mu + sigma*qnorm(u)

```{r}
mu = 8
sigma2 = 3
a = 0
b = 2
iterations_text <- 0
n_text <-NULL
while(length(n_text)<100){
  iterations_text <- iterations_text+1
  u <- runif(1,pnorm((a-mu)/sqrt(sigma2)), pnorm((b-mu)/sqrt(sigma2))  )
  x <- mu + sqrt(sigma2)*qnorm(u)
  if((x<b)&(x>a) ){
    n_text<- rbind(n_text,x)}
}
paste("It took ",iterations_text, " iterations to get a sample of 100 values in the range of (a,b)")


```
```{r}
par(mfrow=c(1,2))
hist(n)
hist(n_text)
```

Both the first sampling algorithm as well as the textbook algorithm have similar distributions on the interval (0,2) which is to be expected, however the first algorithm required over 30,000 iterations whereas the textbook algorithm only required the 100 iterations to get a n = 100 sample. This is because by performing the transformation of a random uniform you are guaranteeing the range of x to be in the desired range rather than randomly sampling from the whole distribution.



Note: for cross-checking your results you can also generate the sample using the
R package truncnorm

























