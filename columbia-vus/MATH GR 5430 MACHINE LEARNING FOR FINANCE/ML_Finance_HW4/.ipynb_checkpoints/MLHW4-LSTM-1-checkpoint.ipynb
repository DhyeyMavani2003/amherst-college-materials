{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "127d41cb-f031-47ed-8182-18b4b4268e67",
    "_uuid": "91fd3340a32112c8484def249676909381220819",
    "id": "s7xVDmV1YeVa"
   },
   "source": [
    "# LSTM model of StockData\n",
    "\n",
    "[Intro to LSTM](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)\n",
    "\n",
    "In this notebook we will go through a basic Long Short Term Memory (LSTM) model for time series. The notebooks does the following things:\n",
    "* First load in the data. The preproccessing only consist of normalization and the creation of windows.\n",
    "* Creation of the LSTM model\n",
    "* Training the LSTM model\n",
    "* Testing the LSTM model with 1 time step and with 1 window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77fcf79e-c42a-42c3-91a4-d20729ba4e16",
    "_uuid": "46ba1ba6434db508c0315c48e7f5ac50895b921b",
    "id": "fn1L6O-tYeVe"
   },
   "source": [
    "## Importing libraries and loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c5a4c6cd-533e-4fc6-ac98-9374c7d17d58",
    "_uuid": "c68269fdda537314957b3632d8d455cfabd29e90",
    "id": "Jk0XHZbhYeVe"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Install packages accordingly if haven't done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HlfkHLJYeVf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import datetime as dt\n",
    "from sklearn import linear_model \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import plotly\n",
    "\n",
    "# import the relevant Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1cd8a99d-6e9f-40fd-893d-12d2dbb2d511",
    "_uuid": "7e20a12ab2c0933ce9a4930713dedf700e9f4a1a",
    "id": "Z1oXex5mYeVh"
   },
   "source": [
    "### 1) Loading in the data\n",
    "\n",
    "Retrieve Close price of Google from yahoo finance from 2006-01-02 to 2022-12-30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93bf734b-6100-4286-974d-f695993a6988",
    "_uuid": "7a8a7801133a86e2158ffa2b8506ce8b47aab130",
    "id": "Ah1L_nY5YeVh"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "goog = yf.Ticker('goog')\n",
    "history = goog.history(interval=\"1d\",\n",
    "            start=\"2006-01-02\", end=\"2022-12-30\", prepost=False, actions=True,\n",
    "            auto_adjust=True, back_adjust=False,\n",
    "            proxy=None, rounding=False)\n",
    "df = history.reset_index()\n",
    "df['Label'] = 'Google'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "13a9d5fa-c074-46db-b7cb-2e6712b9fb51",
    "_uuid": "eab4bb7febbb0db9539d97970e6ea9dfee41df5a",
    "id": "xm18JIfsYeVi"
   },
   "source": [
    "### Visualize the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "VOdiH-WpnCoZ",
    "outputId": "ff130507-638f-4681-8b0a-e6f3608bf7b5"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba0a6406-9e68-49a7-bfcb-5f6335d9837d",
    "_uuid": "e97af0dd991ecc27066dbea3d1a50190491fbe4e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "OIm9ayW8YeVi",
    "outputId": "dc570a72-a268-4667-9129-dc39f2fb53af"
   },
   "outputs": [],
   "source": [
    "plt.plot(df['Date'],df['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64adcccf-c7ff-4f43-9378-5003e7173bab",
    "_uuid": "44526aff819b043d21950821b2117ebd0ec968c8",
    "id": "AXV7pHgyYeVj"
   },
   "source": [
    "### 2) Creating windows and normalizing the data\n",
    "\n",
    "The default window here is 20. The final question will ask you to consider this parameter in your final analysis and how it might impact your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ccaad6cb-0de6-42e0-82ee-818b06debddd",
    "_uuid": "716f887dd7f0d1bed4a2b363b1caf0f70d0799ad",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcklkoJdYeVk",
    "outputId": "26229061-ce7d-42b8-c804-b9ce68cc2500"
   },
   "outputs": [],
   "source": [
    "window_len = 20\n",
    "\n",
    "#Create a data point (i.e. a date) which splits the training and testing set\n",
    "split_date = list(df[\"Date\"][-(2*window_len+1):])[0]\n",
    "print(\"split_date:\",split_date)\n",
    "\n",
    "#Split the training and test set\n",
    "training_set, test_set = df[df['Date'] < split_date], df[df['Date'] >= split_date]\n",
    "training_set = training_set.drop(['Date','Label', 'Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits'], 1)\n",
    "test_set = test_set.drop(['Date','Label', 'Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits'], 1)\n",
    "#Create windows for training\n",
    "LSTM_training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    for col in list(temp_set):\n",
    "        temp_set[col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    \n",
    "    LSTM_training_inputs.append(temp_set)\n",
    "LSTM_training_outputs = (training_set['Close'][window_len:].values/training_set['Close'][:-window_len].values)-1\n",
    "\n",
    "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
    "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
    "\n",
    "#Create windows for testing\n",
    "LSTM_test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    \n",
    "    for col in list(temp_set):\n",
    "        temp_set[col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    \n",
    "    LSTM_test_inputs.append(temp_set)\n",
    "LSTM_test_outputs = (test_set['Close'][window_len:].values/test_set['Close'][:-window_len].values)-1\n",
    "\n",
    "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
    "LSTM_test_inputs = np.array(LSTM_test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f9e074d-b290-49b0-bbd3-08d5776a1405",
    "_uuid": "d5b2f417083fcc4141bad3483672072dcaaebfd7",
    "id": "fdnLVslNYeVl"
   },
   "source": [
    "## 3) LSTM model definition\n",
    "\n",
    "LSTM's have a set of parameters that can be tuned to your data set. Consider these inputs: **activation function, loss function, dropout rate, optimizer, nn layers/architecture** and review your options in the documentation.\n",
    "\n",
    "[Keras Docs](https://keras.io/api/layers/recurrent_layers/lstm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOAQmZ-eYeVn"
   },
   "source": [
    "This is a standard function for building a LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "90d2eb52-a27a-4d44-a54e-e69a9a158497",
    "_uuid": "fa67caf03f6b8add47bf833faacfa1292479f1fc",
    "id": "sx9NBnmNYeVn"
   },
   "outputs": [],
   "source": [
    "def build_model(inputs, output_size, neurons, activ_func=\"tanh\",\n",
    "                dropout=0.10, loss=\"mae\", optimizer=\"adam\"):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ae6a8a52-b167-4849-bd20-60d05e2a6b77",
    "_uuid": "6a62d2b17ee78777e4305b8f0832adb64c7596cd",
    "id": "RLklRSWQYeVo"
   },
   "source": [
    "## 4) Training of the LSTM model\n",
    "\n",
    "Just like most ML models choosing a stopping condition is important. Here we use **Epochs** or iterations to set this stopping condition where we also monitor the loss at each step. Consider **Epochs** as a parameter to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a5a1908-797d-4b34-9eba-a6a52316c243",
    "_uuid": "3f7e8dfd6c9cbd294d7f73e30e8a03b75409dfaf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riGrvYMCYeVo",
    "outputId": "1a65e1e9-4c34-4a20-da67-1d3887d130c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialise model architecture\n",
    "nn_model = build_model(LSTM_training_inputs, output_size=1, neurons = 32)\n",
    "# model output is next price normalised to 10th previous closing price\n",
    "# train model on data\n",
    "# note: eth_history contains information on the training error per epoch\n",
    "nn_history = nn_model.fit(LSTM_training_inputs, LSTM_training_outputs, \n",
    "                            epochs=5, batch_size=1, verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7866dd63-ff05-4fcd-ad77-6aa37b3904a0",
    "_uuid": "fcf55764d5abf3fc0fdb1482a609bccce90afa18",
    "id": "V42I-E9LYeVo"
   },
   "source": [
    "### Plot of prediction of one data point ahead\n",
    "As can be seen in the plot, one step prediction is not bad. The scale is a bit off, because the data is normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f8b3baa-ee8c-4230-909d-4bba49212cc9",
    "_uuid": "974f151cfd79381ea720522c3bcae77c7954002c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "GrBTQGN8YeVp",
    "outputId": "9f45b63c-caea-477e-d6f4-a0da4a06fac2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(LSTM_test_outputs, label = \"actual\")\n",
    "plt.plot(nn_model.predict(LSTM_test_inputs), label = \"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MAE = mean_absolute_error(LSTM_test_outputs, nn_model.predict(LSTM_test_inputs))\n",
    "print('The Mean Absolute Error is: {}'.format(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tb_TdRotlmYE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc86f1a3-f0be-48c0-960f-e1c8812c17c5",
    "_uuid": "af67779607f62291abda55d4185d7617ef79bd6d",
    "id": "MNhhY8O3YeVp"
   },
   "source": [
    "### Prediction of one window (n steps) ahead\n",
    "As can be seen in the plot below, the performance degrades when predicting multiple time points ahead. However, compared to something like linear regression the performance is better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "554ac64d-747e-486e-b242-3d3acea035e7",
    "_uuid": "2b370414a6468a68f970febd3bba61af8c45f7c8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "bAMXt8i3YeVp",
    "outputId": "85839c8e-aa0d-46c0-ffee-7fb07deefc2c"
   },
   "outputs": [],
   "source": [
    "def predict_sequence_full(model, data, window_size):\n",
    "    #Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "    curr_frame = data[0]\n",
    "    predicted = []\n",
    "    for i in range(len(data)):\n",
    "        predicted.append(model.predict(curr_frame[np.newaxis,:,:])[0,0])\n",
    "        curr_frame = curr_frame[1:]\n",
    "        curr_frame = np.insert(curr_frame, [window_size-1], predicted[-1], axis=0)\n",
    "    return predicted\n",
    "\n",
    "predictions = predict_sequence_full(nn_model, LSTM_test_inputs, 20)\n",
    "\n",
    "plt.plot(LSTM_test_outputs, label=\"actual\")\n",
    "plt.plot(predictions, label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "MAE = mean_absolute_error(LSTM_test_outputs, predictions)\n",
    "print('The Mean Absolute Error is: {}'.format(MAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c2af4c2c-0ba7-4bf4-a7e7-428f6147a0ec",
    "_uuid": "c8e77413a56b9a25e6d3ee4653bab73058180eaf",
    "id": "gPxrH_dnYeVp"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "For this HW you will run the code above with the default parameters and understand the logic and flow of the program. Once you are confident the code runs you are to test different parameter settings. You are to report the best set of parameters that you find and explain the importance of each parameter and how it impacts the training of the model. To best know how much to adjust and how to interpret the impacts I suggest changing one parameter at a at a time. This is a manual grid search you are performing so that you can become familiar with each parameter. In the future you can have grid search algorithms find the best set for you. \n",
    "\n",
    "For each define the parameter and its impact to the model. Report the set of values tested and the best parameter setting you found for each.\n",
    "\n",
    "1) Window Length\n",
    "2) LSTM Parameter: activation function\n",
    "3) LSTM Parameter: loss function\n",
    "4) LSTM Parameter: dropout rate\n",
    "5) LSTM Parameter: optimizer\n",
    "6) LSTM Parameter: nn layers/architecture\n",
    "7) Epochs\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
